{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fF8ysCfYKgTP"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "We will take the following steps to implement YOLOv4 on our custom data:\n",
    "* Install TensorFlow2 Object Detection Dependencies\n",
    "* Download Custom TensorFlow2 Object Detection Dataset\n",
    "* Write Custom TensorFlow2 Object Detection Training Configuation\n",
    "* Train Custom TensorFlow2 Object Detection Model\n",
    "* Export Custom TensorFlow2 Object Detection Weights\n",
    "* Use Trained TensorFlow2 Object Detection For Inference on Test Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7EOtpvlLeS0"
   },
   "source": [
    "# Install TensorFlow2 Object Detection Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U --pre tensorflow==\"2.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "\n",
    "# Why is the version not 2.2.0?\n",
    "# Need to use a virtualenvironment to better manage which package used in the kernel?\n",
    "\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: gitpython in /anaconda/envs/py38_default/lib/python3.8/site-packages (3.1.24)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from gitpython) (4.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from gitpython) (3.7.4.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/anaconda/envs/py38_default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gitpython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ypWGYdPlLRUN",
    "outputId": "64d4d196-f81a-4e85-bc6e-a2650e185855"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from git import Repo\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "models_folder = os.path.join(os.getcwd(), \"models\")\n",
    "\n",
    "# Clone the tensorflow models repository if it doesn't already exist in this folder\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "    while \"models\" in pathlib.Path.cwd().parts:\n",
    "        os.chdir('..')\n",
    "\n",
    "    logging.info(\"The models have already been uploaded. Change working directory to the models folder.\")\n",
    "\n",
    "elif not pathlib.Path('models').exists():\n",
    "    os.mkdir(\"./models\")\n",
    "    repo = Repo.clone_from(\n",
    "        'http://RebSolcia:Clementinabookie18121998!@github.com/tensorflow/models.git',\n",
    "        models_folder,\n",
    "        depth=1,\n",
    "        branch='master',\n",
    "    )\n",
    "\n",
    "    logging.info(\"The models have now been loaded from the tensorflow/models.git repo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyCoco library\n",
    "\n",
    "The following steps are needed in order to avoid having problems with Pycoco.\n",
    "\n",
    "1. Clone the official repository\n",
    "2. Navigate to the PythonAPI folder and open the setup.py file\n",
    "3. Edit line 12 to be extra_compile_args=[]. The rationale here is to remove the Clang specific arguments, which don’t work on MVCC.\n",
    "4. On a CMD terminal at the PythonAPI folder, run:\n",
    "python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycoco_folder = os.path.join(os.getcwd(), \"pycoco\")\n",
    "\n",
    "# Clone the pycoco repository if it doesn't exist. It is needed to avoid clashes with the TF2API\n",
    "if \"pycoco\" in pathlib.Path.cwd().parts:\n",
    "    while \"pycoco\" in pathlib.Path.cwd().parts:\n",
    "        os.chdir('..')\n",
    "\n",
    "    logging.info(\"The models have already been uploaded. Change working directory to the models folder.\")\n",
    "\n",
    "elif not pathlib.Path('pycoco').exists():\n",
    "    os.mkdir(\"./pycoco\")\n",
    "    repo = Repo.clone_from(\n",
    "        'http://RebSolcia:Clementinabookie18121998!@github.com/cocodataset/cocoapi.git',\n",
    "        pycoco_folder, \n",
    "        branch=\"master\"\n",
    "    )\n",
    "\n",
    "    logging.info(\"The models have now been loaded from the coco repo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final command will build and install the package within your current environment, ready to go. To test if the installation succeeded, fire up Python and import it as: import pycocotools.\n",
    "You might ask if we shouldn’t have added MVCC specific flags to replace Clang ones. I wondered the same, but it worked nicely, and no error has popped up for me with this. The original Clang flags, from what I can tell, are there to disable some warnings and force C99 compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pycoco/PythonAPI\n",
      "running build_ext\n",
      "skipping 'pycocotools/_mask.c' Cython extension (up-to-date)\n",
      "copying build/lib.linux-x86_64-3.8/pycocotools/_mask.cpython-38-x86_64-linux-gnu.so -> pycocotools\n"
     ]
    }
   ],
   "source": [
    "%cd /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pycoco/PythonAPI\n",
    "!python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF2 API: Changes to the Models folder\n",
    "\n",
    "1. Navigate to “./research/object_detection/packages/tf2/” and edit the setup.py file. From the REQUIRED_PACKAGES list, delete the pycocotools reference (line 20). This change will prevent the installation process from trying to reinstall pycocotools from pip, which would fail and abort the whole process.\n",
    "2. Copy this setup.py file to the “./research” folder, replacing the setup.py that was already there.\n",
    "3. With the CMD open at the research folder, compile protocol buffers with:\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "4. If the previous command worked, nothing should appear. Not the most intuitive of things, I know. After it, run the following:\n",
    "python -m pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QPmVBSlLTzM",
    "outputId": "32e07c91-e6fc-432b-b41a-2885a86f5985",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "Processing /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: avro-python3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (1.9.2.1)\n",
      "Requirement already satisfied: apache-beam in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (2.33.0)\n",
      "Requirement already satisfied: pillow in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (8.3.1)\n",
      "Requirement already satisfied: lxml in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (4.6.3)\n",
      "Requirement already satisfied: matplotlib in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (3.3.4)\n",
      "Requirement already satisfied: Cython in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (0.29.23)\n",
      "Requirement already satisfied: contextlib2 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (21.6.0)\n",
      "Requirement already satisfied: tf-slim in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (1.1.0)\n",
      "Requirement already satisfied: six in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (1.15.0)\n",
      "Requirement already satisfied: lvis in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (0.5.3)\n",
      "Requirement already satisfied: scipy in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (1.4.1)\n",
      "Requirement already satisfied: pandas in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (1.2.5)\n",
      "Requirement already satisfied: tf-models-official>=2.5.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (2.6.0)\n",
      "Requirement already satisfied: tensorflow_io in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (0.21.0)\n",
      "Requirement already satisfied: keras==2.6.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from object-detection==0.1) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.21.4)\n",
      "Requirement already satisfied: sentencepiece in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.1.96)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.8.0)\n",
      "Requirement already satisfied: oauth2client in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
      "Requirement already satisfied: sacrebleu in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.0.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.4.0)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.28.0)\n",
      "Requirement already satisfied: tensorflow>=2.5.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.1)\n",
      "Requirement already satisfied: tensorflow-addons in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.14.0)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.7.0)\n",
      "Requirement already satisfied: tensorflow-text>=2.5.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
      "Requirement already satisfied: opencv-python-headless in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.5.4.58)\n",
      "Requirement already satisfied: gin-config in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
      "Requirement already satisfied: pycocotools in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.0.2)\n",
      "Requirement already satisfied: seqeval in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.2.2)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (8.0.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.19.1)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=1.21.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.2.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.32.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.1.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.53.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.25.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.7.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.4.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi in /anaconda/envs/py38_default/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil in /anaconda/envs/py38_default/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.1)\n",
      "Requirement already satisfied: urllib3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.26.6)\n",
      "Requirement already satisfied: python-slugify in /anaconda/envs/py38_default/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (5.0.2)\n",
      "Requirement already satisfied: tqdm in /anaconda/envs/py38_default/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.61.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from pandas->object-detection==0.1) (2021.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.0.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.36.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.7 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.2.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.7.0)\n",
      "Requirement already satisfied: clang~=5.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (5.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12.1)\n",
      "Collecting h5py~=3.1.0\n",
      "  Using cached h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12)\n",
      "Collecting numpy>=1.15.4\n",
      "  Using cached numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.41.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.0.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.6)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (3.12.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (1.4.6)\n",
      "Requirement already satisfied: orjson<4.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (3.6.4)\n",
      "Requirement already satisfied: pyarrow<5.0.0,>=0.15.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (4.0.1)\n",
      "Requirement already satisfied: future<1.0.0,>=0.18.2 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (0.18.2)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (2.6.0)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (1.4.2)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (0.3.1.1)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (1.7)\n",
      "Requirement already satisfied: docopt in /anaconda/envs/py38_default/lib/python3.8/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cycler>=0.10.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from lvis->object-detection==0.1) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.1.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from lvis->object-detection==0.1) (1.3.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from lvis->object-detection==0.1) (4.5.3.56)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
      "Requirement already satisfied: colorama in /anaconda/envs/py38_default/lib/python3.8/site-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.4.4)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n",
      "Requirement already satisfied: regex in /anaconda/envs/py38_default/lib/python3.8/site-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2021.7.6)\n",
      "Requirement already satisfied: portalocker in /anaconda/envs/py38_default/lib/python3.8/site-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2.3.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.13.0)\n",
      "Requirement already satisfied: promise in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
      "Requirement already satisfied: tensorflow-metadata in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.4.0)\n",
      "Requirement already satisfied: importlib-resources in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.3.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from importlib-resources->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.21.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from tensorflow_io->object-detection==0.1) (0.21.0)\n",
      "Building wheels for collected packages: object-detection\n",
      "  Building wheel for object-detection (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1679000 sha256=cc6987c3f5fc33b0e0d9572ba16b044db8dc7d37f98ca337e0db2acc0a94127d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-j_mu11xt/wheels/a4/27/31/b41a2f9b118ebb35237b34adc3f408b0c60bd7f122d0a7eb79\n",
      "Successfully built object-detection\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "Installing collected packages: numpy, h5py, object-detection\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "    Found existing installation: numpy 1.21.4\n",
      "    Uninstalling numpy-1.21.4:\n",
      "      Successfully uninstalled numpy-1.21.4\n",
      "  Attempting uninstall: h5py\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "    Found existing installation: h5py 2.9.0\n",
      "    Uninstalling h5py-2.9.0:\n",
      "      Successfully uninstalled h5py-2.9.0\n",
      "  Attempting uninstall: object-detection\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "    Found existing installation: object-detection 0.1\n",
      "    Uninstalling object-detection-0.1:\n",
      "      Successfully uninstalled object-detection-0.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fbprophet 0.7.1 requires cmdstanpy==0.9.5, which is not installed.\n",
      "fbprophet 0.7.1 requires setuptools-git>=1.2, which is not installed.\n",
      "tensorflow-gpu 2.5.0 requires grpcio~=1.34.0, but you have grpcio 1.41.1 which is incompatible.\n",
      "tensorflow-gpu 2.5.0 requires tensorflow-estimator<2.6.0,>=2.5.0rc0, but you have tensorflow-estimator 2.2.0 which is incompatible.\n",
      "networkx 2.6.1 requires scipy!=1.6.1,>=1.5, but you have scipy 1.4.1 which is incompatible.\u001b[0m\n",
      "Successfully installed h5py-3.1.0 numpy-1.19.5 object-detection-0.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/anaconda/envs/py38_default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "%cp /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/object_detection/packages/tf2/setup.py .\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure tu uninstall h5py and re-install it in the 2.9 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\r\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\r\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\r\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\r\n",
      "Found existing installation: h5py 3.1.0\r\n",
      "Uninstalling h5py-3.1.0:\r\n",
      "  Successfully uninstalled h5py-3.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall h5py -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "Collecting h5py==2.9\n",
      "  Using cached h5py-2.9.0-cp38-cp38-manylinux1_x86_64.whl (2.8 MB)\n",
      "Requirement already satisfied: six in /anaconda/envs/py38_default/lib/python3.8/site-packages (from h5py==2.9) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from h5py==2.9) (1.19.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "Installing collected packages: h5py\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.6.1 requires h5py~=3.1.0, but you have h5py 2.9.0 which is incompatible.\n",
      "tensorflow-gpu 2.5.0 requires grpcio~=1.34.0, but you have grpcio 1.41.1 which is incompatible.\n",
      "tensorflow-gpu 2.5.0 requires h5py~=3.1.0, but you have h5py 2.9.0 which is incompatible.\n",
      "tensorflow-gpu 2.5.0 requires tensorflow-estimator<2.6.0,>=2.5.0rc0, but you have tensorflow-estimator 2.2.0 which is incompatible.\u001b[0m\n",
      "Successfully installed h5py-2.9.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/anaconda/envs/py38_default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py==2.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire the model up\n",
    "\n",
    "Once everything is installed, import all the libraries that are needed and launch a sample training to check that everything works smoothly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wHfsJ5nWLWh9"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "import imageio\n",
    "import glob\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from models.research.object_detection.utils import label_map_util\n",
    "from models.research.object_detection.utils import config_util\n",
    "from models.research.object_detection.utils import visualization_utils as viz_utils\n",
    "from models.research.object_detection.builders import model_builder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a pip freeze to see whether tensorflow-gpu is installed, and run the test to see everything works smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh_HPMOqWH9z",
    "outputId": "31063c4e-d8f1-484e-c325-005553c763eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests under Python 3.8.10: /anaconda/envs/py38_default/bin/python\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "2021-11-17 14:29:41.150092: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-17 14:30:01.115639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11567 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0001:00:00.0, compute capability: 3.7\n",
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/builders/model_builder.py:1100: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(('Building experimental DeepMAC meta-arch.'\n",
      "W1117 14:30:01.711951 140271730762688 model_builder.py:1100] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 20.98s\n",
      "I1117 14:30:01.975943 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 20.98s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 1.03s\n",
      "I1117 14:30:03.009073 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 1.03s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.28s\n",
      "I1117 14:30:03.288333 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.28s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.25s\n",
      "I1117 14:30:03.534816 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.25s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 2.52s\n",
      "I1117 14:30:06.055242 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 2.52s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
      "I1117 14:30:06.056344 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
      "I1117 14:30:06.078710 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
      "I1117 14:30:06.093168 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
      "I1117 14:30:06.108011 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.1s\n",
      "I1117 14:30:06.206812 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.1s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.09s\n",
      "I1117 14:30:06.298403 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.09s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.1s\n",
      "I1117 14:30:06.396288 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.1s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.1s\n",
      "I1117 14:30:06.497389 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.1s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.09s\n",
      "I1117 14:30:06.590838 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.09s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n",
      "I1117 14:30:06.617721 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "I1117 14:30:06.790510 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
      "I1117 14:30:06.790710 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n",
      "I1117 14:30:06.790790 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 3\n",
      "I1117 14:30:06.792905 140271730762688 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1117 14:30:06.809273 140271730762688 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1117 14:30:06.809370 140271730762688 efficientnet_model.py:147] round_filter input=16 output=16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1117 14:30:06.866880 140271730762688 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1117 14:30:06.866980 140271730762688 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1117 14:30:07.017105 140271730762688 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1117 14:30:07.017230 140271730762688 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1117 14:30:07.164209 140271730762688 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1117 14:30:07.164330 140271730762688 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1117 14:30:07.535388 140271730762688 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1117 14:30:07.535560 140271730762688 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1117 14:30:07.758821 140271730762688 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1117 14:30:07.758981 140271730762688 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1117 14:30:08.055815 140271730762688 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1117 14:30:08.055957 140271730762688 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I1117 14:30:08.126254 140271730762688 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I1117 14:30:08.154254 140271730762688 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1117 14:30:08.204369 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
      "I1117 14:30:08.204482 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 88\n",
      "I1117 14:30:08.204586 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 4\n",
      "I1117 14:30:08.206148 140271730762688 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1117 14:30:08.220892 140271730762688 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1117 14:30:08.220992 140271730762688 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1117 14:30:08.338523 140271730762688 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1117 14:30:08.338639 140271730762688 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1117 14:30:08.559335 140271730762688 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1117 14:30:08.559489 140271730762688 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1117 14:30:08.787644 140271730762688 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1117 14:30:08.787806 140271730762688 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1117 14:30:09.090323 140271730762688 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1117 14:30:09.090484 140271730762688 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1117 14:30:09.385509 140271730762688 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1117 14:30:09.385679 140271730762688 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1117 14:30:09.756130 140271730762688 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1117 14:30:09.756286 140271730762688 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I1117 14:30:09.902303 140271730762688 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I1117 14:30:09.928673 140271730762688 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1117 14:30:09.986665 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
      "I1117 14:30:09.986769 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 112\n",
      "I1117 14:30:09.986839 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 5\n",
      "I1117 14:30:09.988345 140271730762688 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1117 14:30:10.002517 140271730762688 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1117 14:30:10.002624 140271730762688 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1117 14:30:10.114878 140271730762688 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1117 14:30:10.114978 140271730762688 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1117 14:30:10.328398 140271730762688 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1117 14:30:10.328517 140271730762688 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1117 14:30:10.542712 140271730762688 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1117 14:30:10.542845 140271730762688 efficientnet_model.py:147] round_filter input=80 output=88\n",
      "I1117 14:30:10.828949 140271730762688 efficientnet_model.py:147] round_filter input=80 output=88\n",
      "I1117 14:30:10.829121 140271730762688 efficientnet_model.py:147] round_filter input=112 output=120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1117 14:30:11.291751 140271730762688 efficientnet_model.py:147] round_filter input=112 output=120\n",
      "I1117 14:30:11.291911 140271730762688 efficientnet_model.py:147] round_filter input=192 output=208\n",
      "I1117 14:30:11.660157 140271730762688 efficientnet_model.py:147] round_filter input=192 output=208\n",
      "I1117 14:30:11.660293 140271730762688 efficientnet_model.py:147] round_filter input=320 output=352\n",
      "I1117 14:30:11.804625 140271730762688 efficientnet_model.py:147] round_filter input=1280 output=1408\n",
      "I1117 14:30:11.832585 140271730762688 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1117 14:30:11.889590 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
      "I1117 14:30:11.889689 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 160\n",
      "I1117 14:30:11.889758 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 6\n",
      "I1117 14:30:11.891267 140271730762688 efficientnet_model.py:147] round_filter input=32 output=40\n",
      "I1117 14:30:11.905598 140271730762688 efficientnet_model.py:147] round_filter input=32 output=40\n",
      "I1117 14:30:11.905690 140271730762688 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1117 14:30:12.019258 140271730762688 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1117 14:30:12.019361 140271730762688 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1117 14:30:12.236157 140271730762688 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1117 14:30:12.236288 140271730762688 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1117 14:30:12.449427 140271730762688 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1117 14:30:12.449567 140271730762688 efficientnet_model.py:147] round_filter input=80 output=96\n",
      "I1117 14:30:12.813308 140271730762688 efficientnet_model.py:147] round_filter input=80 output=96\n",
      "I1117 14:30:12.813464 140271730762688 efficientnet_model.py:147] round_filter input=112 output=136\n",
      "I1117 14:30:13.173676 140271730762688 efficientnet_model.py:147] round_filter input=112 output=136\n",
      "I1117 14:30:13.173829 140271730762688 efficientnet_model.py:147] round_filter input=192 output=232\n",
      "I1117 14:30:13.618644 140271730762688 efficientnet_model.py:147] round_filter input=192 output=232\n",
      "I1117 14:30:13.618803 140271730762688 efficientnet_model.py:147] round_filter input=320 output=384\n",
      "I1117 14:30:13.760889 140271730762688 efficientnet_model.py:147] round_filter input=1280 output=1536\n",
      "I1117 14:30:13.787356 140271730762688 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1117 14:30:13.849644 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
      "I1117 14:30:13.849744 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 224\n",
      "I1117 14:30:13.849813 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 7\n",
      "I1117 14:30:13.851311 140271730762688 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1117 14:30:13.865994 140271730762688 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1117 14:30:13.866101 140271730762688 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1117 14:30:13.979049 140271730762688 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1117 14:30:13.979148 140271730762688 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1117 14:30:14.263440 140271730762688 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1117 14:30:14.263580 140271730762688 efficientnet_model.py:147] round_filter input=40 output=56\n",
      "I1117 14:30:14.553455 140271730762688 efficientnet_model.py:147] round_filter input=40 output=56\n",
      "I1117 14:30:14.553750 140271730762688 efficientnet_model.py:147] round_filter input=80 output=112\n",
      "I1117 14:30:15.239469 140271730762688 efficientnet_model.py:147] round_filter input=80 output=112\n",
      "I1117 14:30:15.239645 140271730762688 efficientnet_model.py:147] round_filter input=112 output=160\n",
      "I1117 14:30:15.686338 140271730762688 efficientnet_model.py:147] round_filter input=112 output=160\n",
      "I1117 14:30:15.686507 140271730762688 efficientnet_model.py:147] round_filter input=192 output=272\n",
      "I1117 14:30:16.293527 140271730762688 efficientnet_model.py:147] round_filter input=192 output=272\n",
      "I1117 14:30:16.293686 140271730762688 efficientnet_model.py:147] round_filter input=320 output=448\n",
      "I1117 14:30:16.436931 140271730762688 efficientnet_model.py:147] round_filter input=1280 output=1792\n",
      "I1117 14:30:16.463688 140271730762688 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1117 14:30:16.538998 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
      "I1117 14:30:16.539172 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 288\n",
      "I1117 14:30:16.539244 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 7\n",
      "I1117 14:30:16.540906 140271730762688 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1117 14:30:16.556143 140271730762688 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1117 14:30:16.556233 140271730762688 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1117 14:30:16.729480 140271730762688 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1117 14:30:16.729623 140271730762688 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1117 14:30:17.095922 140271730762688 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1117 14:30:17.096084 140271730762688 efficientnet_model.py:147] round_filter input=40 output=64\n",
      "I1117 14:30:17.465558 140271730762688 efficientnet_model.py:147] round_filter input=40 output=64\n",
      "I1117 14:30:17.465711 140271730762688 efficientnet_model.py:147] round_filter input=80 output=128\n",
      "I1117 14:30:17.979047 140271730762688 efficientnet_model.py:147] round_filter input=80 output=128\n",
      "I1117 14:30:17.979201 140271730762688 efficientnet_model.py:147] round_filter input=112 output=176\n",
      "I1117 14:30:18.493031 140271730762688 efficientnet_model.py:147] round_filter input=112 output=176\n",
      "I1117 14:30:18.493187 140271730762688 efficientnet_model.py:147] round_filter input=192 output=304\n",
      "I1117 14:30:19.156646 140271730762688 efficientnet_model.py:147] round_filter input=192 output=304\n",
      "I1117 14:30:19.156799 140271730762688 efficientnet_model.py:147] round_filter input=320 output=512\n",
      "I1117 14:30:19.374433 140271730762688 efficientnet_model.py:147] round_filter input=1280 output=2048\n",
      "I1117 14:30:19.401163 140271730762688 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1117 14:30:19.712227 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
      "I1117 14:30:19.712386 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n",
      "I1117 14:30:19.712456 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 8\n",
      "I1117 14:30:19.714053 140271730762688 efficientnet_model.py:147] round_filter input=32 output=56\n",
      "I1117 14:30:19.729012 140271730762688 efficientnet_model.py:147] round_filter input=32 output=56\n",
      "I1117 14:30:19.729105 140271730762688 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1117 14:30:19.907126 140271730762688 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1117 14:30:19.907269 140271730762688 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1117 14:30:20.351997 140271730762688 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1117 14:30:20.352150 140271730762688 efficientnet_model.py:147] round_filter input=40 output=72\n",
      "I1117 14:30:20.789897 140271730762688 efficientnet_model.py:147] round_filter input=40 output=72\n",
      "I1117 14:30:20.790069 140271730762688 efficientnet_model.py:147] round_filter input=80 output=144\n",
      "I1117 14:30:21.385386 140271730762688 efficientnet_model.py:147] round_filter input=80 output=144\n",
      "I1117 14:30:21.385559 140271730762688 efficientnet_model.py:147] round_filter input=112 output=200\n",
      "I1117 14:30:21.977173 140271730762688 efficientnet_model.py:147] round_filter input=112 output=200\n",
      "I1117 14:30:21.977332 140271730762688 efficientnet_model.py:147] round_filter input=192 output=344\n",
      "I1117 14:30:22.797155 140271730762688 efficientnet_model.py:147] round_filter input=192 output=344\n",
      "I1117 14:30:22.797312 140271730762688 efficientnet_model.py:147] round_filter input=320 output=576\n",
      "I1117 14:30:23.019819 140271730762688 efficientnet_model.py:147] round_filter input=1280 output=2304\n",
      "I1117 14:30:23.048003 140271730762688 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1117 14:30:23.143192 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
      "I1117 14:30:23.143333 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n",
      "I1117 14:30:23.143405 140271730762688 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 8\n",
      "I1117 14:30:23.145037 140271730762688 efficientnet_model.py:147] round_filter input=32 output=64\n",
      "I1117 14:30:23.160289 140271730762688 efficientnet_model.py:147] round_filter input=32 output=64\n",
      "I1117 14:30:23.160390 140271730762688 efficientnet_model.py:147] round_filter input=16 output=32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1117 14:30:23.394538 140271730762688 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1117 14:30:23.394654 140271730762688 efficientnet_model.py:147] round_filter input=24 output=48\n",
      "I1117 14:30:23.911428 140271730762688 efficientnet_model.py:147] round_filter input=24 output=48\n",
      "I1117 14:30:23.911602 140271730762688 efficientnet_model.py:147] round_filter input=40 output=80\n",
      "I1117 14:30:24.731386 140271730762688 efficientnet_model.py:147] round_filter input=40 output=80\n",
      "I1117 14:30:24.731559 140271730762688 efficientnet_model.py:147] round_filter input=80 output=160\n",
      "I1117 14:30:25.465126 140271730762688 efficientnet_model.py:147] round_filter input=80 output=160\n",
      "I1117 14:30:25.465283 140271730762688 efficientnet_model.py:147] round_filter input=112 output=224\n",
      "I1117 14:30:26.212736 140271730762688 efficientnet_model.py:147] round_filter input=112 output=224\n",
      "I1117 14:30:26.212894 140271730762688 efficientnet_model.py:147] round_filter input=192 output=384\n",
      "I1117 14:30:27.186359 140271730762688 efficientnet_model.py:147] round_filter input=192 output=384\n",
      "I1117 14:30:27.186534 140271730762688 efficientnet_model.py:147] round_filter input=320 output=640\n",
      "I1117 14:30:27.484805 140271730762688 efficientnet_model.py:147] round_filter input=1280 output=2560\n",
      "I1117 14:30:27.512262 140271730762688 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 21.01s\n",
      "I1117 14:30:27.627075 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 21.01s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
      "I1117 14:30:27.637588 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
      "I1117 14:30:27.639248 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
      "I1117 14:30:27.639684 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
      "I1117 14:30:27.641202 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "[ RUN      ] ModelBuilderTF2Test.test_session\n",
      "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
      "I1117 14:30:27.642583 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
      "I1117 14:30:27.642986 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
      "I1117 14:30:27.643984 140271730762688 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "----------------------------------------------------------------------\n",
      "Ran 24 tests in 46.650s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "#run model builder test\n",
    "!python /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/object_detection/builders/model_builder_tf2_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change current directory to be sure everything works smoothly. This process of directory change will take place often to ensure code compatibility when constructing paths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data\n",
    "\n",
    "Eventually, remember to change the names of the files so that they are compatible with yours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training\n"
     ]
    }
   ],
   "source": [
    "%cd /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pictures/output_tfrecords/train/merged_logos.tfrecord\n",
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pictures/output_tfrecords/train/logos_label_map.pbtxt\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "if 'output_tfrecords' not in os.listdir(\"pictures\"):\n",
    "    with zipfile.ZipFile(\"./pictures/output_tfrecords.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"./pictures/\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "picture_files_directory = \"/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pictures/\"\n",
    "\n",
    "test_record_fname = os.path.join(picture_files_directory,\"output_tfrecords/valid/merged_logos.tfrecord\")\n",
    "train_record_fname = os.path.join(picture_files_directory,\"output_tfrecords/train/merged_logos.tfrecord\")\n",
    "label_map_pbtxt_fname = os.path.join(picture_files_directory, \"output_tfrecords/train/logos_label_map.pbtxt\")\n",
    "\n",
    "print(train_record_fname,label_map_pbtxt_fname, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2MAcgJ53STW"
   },
   "source": [
    "# Configure Custom TensorFlow2 Object Detection Training Configuration\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> In this section you can specify any model in the [TF2 OD model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) and set up your training configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training\n"
     ]
    }
   ],
   "source": [
    "%cd /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: prettyprinter in /anaconda/envs/py38_default/lib/python3.8/site-packages (0.18.0)\n",
      "Requirement already satisfied: colorful>=0.4.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from prettyprinter) (0.5.4)\n",
      "Requirement already satisfied: Pygments>=2.2.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from prettyprinter) (2.9.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/anaconda/envs/py38_default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install prettyprinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training\n"
     ]
    }
   ],
   "source": [
    "%cd /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gN0EUEa3e5Un"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    'efficientdet-d0': {\n",
      "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
      "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
      "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
      "        'batch_size': 16\n",
      "    },\n",
      "    'faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8': {\n",
      "        'model_name': 'faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8',\n",
      "        'base_pipeline_file': 'faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.config',\n",
      "        'pretrained_checkpoint': 'faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8.tar.gz',\n",
      "        'batch_size': 2\n",
      "    },\n",
      "    'ssd_resnet152_v1': {\n",
      "        'model_name': 'ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8',\n",
      "        'base_pipeline_file': 'ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8.config',\n",
      "        'pretrained_checkpoint': 'ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8.tar.gz',\n",
      "        'batch_size': 2\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: import model settings\n",
    "# For each model, this file returns important info to actually use the model\n",
    "\n",
    "from ModelSettings import Model_Setting\n",
    "from prettyprinter import pprint\n",
    "\n",
    "MODELS_CONFIG = Model_Setting()\n",
    "\n",
    "pprint(MODELS_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: chose the model and extract relevant info\n",
    "\n",
    "chosen_model = 'efficientdet-d0'\n",
    "\n",
    "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
    "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
    "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']\n",
    "batch_size = MODELS_CONFIG[chosen_model]['batch_size'] #if you can fit a large batch in memory, it may speed up your training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The more steps, the longer the training. \n",
    "# Increase if your loss function is still decreasing and validation metrics are increasing. \n",
    "num_steps = 400000\n",
    "\n",
    "#Perform evaluation after so many steps\n",
    "num_eval_steps = 3000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy folder structure\n",
    "\n",
    "Inside the \"research\" folder, we will create a \"deploy\" folder in which we will dump all the data related to the model used and its specific configuration. <br>\n",
    "For this reason, the structure of the deploy folder is as follows:\n",
    "\n",
    "Deploy:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Model A:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Config 1<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Config 2<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...<br>\n",
    "\n",
    "This means that, once we choose a model:\n",
    "1. if there is no folder within \"deploy\" with the model name, then create it and create the the Config 1 folder within the model folder.\n",
    "2. if there is a folder with the name of the model, we need to check if the configurtion of the current model is the same as the one in the folder. If not, create a new Config folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research\n"
     ]
    }
   ],
   "source": [
    "%cd /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it does not exist already, create the 'deploy' folder inside training/models/research\n",
    "\n",
    "main_deploy_folder = '/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/deploy'\n",
    "\n",
    "if \"deploy\" not in os.listdir(os.getcwd()):\n",
    "    os.mkdir(main_deploy_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_configs_for_model(chosen_model):\n",
    "    \n",
    "    # This is a dict with config folder names as keys and values of the config as values\n",
    "    folder_to_values = dict()\n",
    "    \n",
    "    model_path = os.path.join(main_deploy_folder, chosen_model)\n",
    "    \n",
    "    for config_folder in os.listdir(model_path):\n",
    "        if not config_folder == \".ipynb_checkpoints\":\n",
    "            print(config_folder)\n",
    "            config_path = os.path.join(model_path, config_folder)\n",
    "\n",
    "            config_file = os.path.join(config_path,r'pipeline_file.config')\n",
    "            config_values = list()\n",
    "\n",
    "            with open(config_file) as f:\n",
    "                file = f.read()\n",
    "\n",
    "                # Extract all values except the path of the data\n",
    "                # This mean that if we train the same config of a model on a different version of the data, this will overwrite the results\n",
    "                # TODO: add path of the data as well?\n",
    "                # TODO: add fine tune check points?\n",
    "                config_values.append(re.search('batch_size: [0-9]+', file).group()[len('batch_size: '):])\n",
    "                config_values.append(re.search('num_steps: [0-9]+', file).group()[len('num_steps: '):])\n",
    "                config_values.append(re.search('num_classes: [0-9]+', file).group()[len('num_classes: '):])\n",
    "\n",
    "            folder_to_values[config_folder] = config_values\n",
    "        \n",
    "    return folder_to_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "b_ki9jOqxn7V"
   },
   "outputs": [],
   "source": [
    "def get_num_classes(pbtxt_fname):\n",
    "    from object_detection.utils import label_map_util\n",
    "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
    "    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=90, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    return len(category_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16', '400000', '9']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_config = [\n",
    "    str(batch_size),\n",
    "    str(num_steps),\n",
    "    str(num_classes)\n",
    "]\n",
    "\n",
    "current_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model_name = 'ModelA'\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model_name = 'ModelA'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_config = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'efficientdet-d0'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model never used, then create folder for the model and for the current config, the latter inside the former\n",
    "\n",
    "def update_repo_structure(chosen_model):\n",
    "    \n",
    "    model_folder = main_deploy_folder + '/' + chosen_model\n",
    "    \n",
    "    # TODO: it has to be folder, not file\n",
    "    if chosen_model not in os.listdir(main_deploy_folder):\n",
    "        # Case 1: model never used\n",
    "        os.mkdir(model_folder)\n",
    "\n",
    "        config_folder = model_folder + '/config_1'\n",
    "        os.mkdir(config_folder)\n",
    "\n",
    "        print('case1')\n",
    "        print(config_folder)\n",
    "\n",
    "    else:\n",
    "        # Case 2: model already used\n",
    "\n",
    "        list_configs = extract_configs_for_model(chosen_model)\n",
    "        print(list_configs)\n",
    "\n",
    "        if current_config in list(list_configs.values()):\n",
    "            \n",
    "            # Case A: Specifics configs per model already used\n",
    "            for key in list(list_configs.keys()):\n",
    "                if list_configs[key] == current_config:\n",
    "                    config_folder = key\n",
    "                    print('case a')\n",
    "                    print(config_folder)\n",
    "\n",
    "        else:\n",
    "            # Case B: new configs\n",
    "            config_folder = model_folder + f'/config_{len(list_configs)+1}'\n",
    "            os.mkdir(config_folder)\n",
    "            print('case b')\n",
    "            print(config_folder)\n",
    "            \n",
    "    return config_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_1\n",
      "config_6\n",
      "config_3\n",
      "config_4\n",
      "config_2\n",
      "config_5\n",
      "{'config_1': ['15', '40010', '9'], 'config_6': ['16', '400000', '9'], 'config_3': ['15', '40010', '9'], 'config_4': ['16', '40080', '9'], 'config_2': ['13', '40020', '9'], 'config_5': ['16', '40100', '9']}\n",
      "case a\n",
      "config_6\n"
     ]
    }
   ],
   "source": [
    "# Obtain the proper config folder to use in the next cells \n",
    "\n",
    "config_subfolder = update_repo_structure(chosen_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/deploy/efficientdet-d0/config_6'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_folder = os.path.join(os.path.join(main_deploy_folder, chosen_model),config_subfolder)\n",
    "config_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kG4TmJUVrYQ7",
    "outputId": "a7a90aca-9383-4007-b6fb-02729ee02e37"
   },
   "outputs": [],
   "source": [
    "# Step 3.a: using info from step 2, download the weights of the model\n",
    "\n",
    "import tarfile\n",
    "import requests\n",
    "\n",
    "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
    "\n",
    "file_to_be_opened = os.path.join(config_folder, pretrained_checkpoint)\n",
    "\n",
    "# Unzip the tar.gz\n",
    "response = requests.get(download_tar, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(file_to_be_opened, 'wb') as f:\n",
    "        f.write(response.raw.read())\n",
    "\n",
    "tar = tarfile.open(file_to_be_opened)\n",
    "tar.extractall(path=config_folder)\n",
    "tar.close()\n",
    "\n",
    "# TODO: once the tar has been extracted, delete the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-nqYZtdtsgG",
    "outputId": "48f68a17-6ec5-4436-8ff0-a4d549186709"
   },
   "outputs": [],
   "source": [
    "# Step 3.b: using info from step 2, download base training configuration file\n",
    "\n",
    "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
    "\n",
    "abrir = os.path.join(config_folder, base_pipeline_file)\n",
    "\n",
    "response = requests.get(download_config, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(abrir, 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/deploy/efficientdet-d0/config_6/ssd_efficientdet_d0_512x512_coco17_tpu-8.config\n",
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/deploy/efficientdet-d0/config_6/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0\n"
     ]
    }
   ],
   "source": [
    "pipeline_fname = os.path.join(config_folder, base_pipeline_file)\n",
    "print(pipeline_fname)\n",
    "\n",
    "fine_tune_checkpoint = os.path.join(config_folder, model_name,\"checkpoint\", \"ckpt-0\")\n",
    "print(fine_tune_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eA5ht3_yukT",
    "outputId": "fe9716f3-ed98-4a17-f80c-83660b05bdfe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Written fine tune checkpoint\n",
      "INFO:root:Written input path\n",
      "INFO:root:Written label map\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing custom configuration file\n"
     ]
    }
   ],
   "source": [
    "#write custom configuration file by slotting our dataset, model checkpoint, and training parameters into the base pipeline file\n",
    "\n",
    "import re\n",
    "\n",
    "print('writing custom configuration file')\n",
    "\n",
    "with open(pipeline_fname) as f:\n",
    "    s = f.read()\n",
    "\n",
    "with open(os.path.join(config_folder, r'pipeline_file.config'), 'w') as f:\n",
    "    \n",
    "    # fine_tune_checkpoint\n",
    "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "               f'fine_tune_checkpoint: \"{fine_tune_checkpoint}\"', s)\n",
    "    \n",
    "    logging.info(\"Written fine tune checkpoint\")\n",
    "    \n",
    "    # tfrecord files train and test.\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', f'input_path: \"{train_record_fname}\"', s)\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', f'input_path: \"{test_record_fname}\"', s)\n",
    "    \n",
    "    logging.info(\"Written input path\")\n",
    "\n",
    "    # label_map_path\n",
    "    s = re.sub(\n",
    "        'label_map_path: \".*?\"', f'label_map_path: \"{label_map_pbtxt_fname}\"', s)\n",
    "    \n",
    "    logging.info(\"Written label map\")\n",
    "\n",
    "    # Set training batch_size.\n",
    "    s = re.sub('batch_size: [0-9]+',\n",
    "               f'batch_size: {batch_size}', s)\n",
    "\n",
    "    # Set training steps, num_steps\n",
    "    s = re.sub('num_steps: [0-9]+',\n",
    "               f'num_steps: {num_steps}', s)\n",
    "    \n",
    "    # Set number of classes num_classes.\n",
    "    s = re.sub('num_classes: [0-9]+',\n",
    "               f'num_classes: {num_classes}', s)\n",
    "    \n",
    "    # Set number of classes num_classes.\n",
    "    s = re.sub('learning_rate_base: [a-z.0-9-]+',\n",
    "               f'learning_rate_base: 0.08', s)\n",
    "    \n",
    "    # Set number of classes num_classes.\n",
    "    s = re.sub('warmup_learning_rate: [a-z.0-9-]+',\n",
    "               f'warmup_learning_rate: 0.001', s)\n",
    "    \n",
    "    #fine-tune checkpoint type\n",
    "    s = re.sub(\n",
    "        'fine_tune_checkpoint_type: \"classification\"', \n",
    "        'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
    "    \n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training\n"
     ]
    }
   ],
   "source": [
    "%cd /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/deploy/efficientdet-d0/config_6/pipeline_file.config'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_file = os.path.join(config_folder, 'pipeline_file.config')\n",
    "pipeline_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "GMlaN3rs3zLe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The directory TENSOR_RESULTS is already present, files will be stored there\n"
     ]
    }
   ],
   "source": [
    "if \"TENSOR_RESULTS\" not in os.listdir(os.getcwd()):\n",
    "    os.mkdir(os.path.join(os.getcwd(),\"TENSOR_RESULTS\"))\n",
    "    logging.info(\"Creating the directory TENSOR_RESULTS because it did not exist\") \n",
    "else:\n",
    "    logging.info(\"The directory TENSOR_RESULTS is already present, files will be stored there\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The folder model_run_directory WAS ALREADY PRESENT and is set to be: \n",
      " /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS/efficientdet-d0\n",
      "INFO:root:The folder model_dir WAS ALREADY PRESENT and is set to be: \n",
      " /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS/efficientdet-d0/config_6\n"
     ]
    }
   ],
   "source": [
    "tensor_results_directory = '/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS'\n",
    "\n",
    "model_run_directory = os.path.join('/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS',\n",
    "                                  chosen_model)\n",
    "\n",
    "if chosen_model not in os.listdir(tensor_results_directory):\n",
    "    try:\n",
    "        os.mkdir(os.path.join(tensor_results_directory, chosen_model))\n",
    "        logging.info(f\"The folder model_run_directory is set to be: \\n {model_run_directory}\")\n",
    "    except FileExistsError:\n",
    "        logging.info(f\"FILEEXISTSERROR: The folder model_run_directory is set to be: \\n {model_run_directory}\")\n",
    "else:\n",
    "    logging.info(f\"The folder model_run_directory WAS ALREADY PRESENT and is set to be: \\n {model_run_directory}\")\n",
    "\n",
    "model_dir = os.path.join(model_run_directory, config_subfolder.split(\"/\")[-1])\n",
    "\n",
    "if config_subfolder.split(\"/\")[-1] not in os.listdir(model_run_directory):\n",
    "    try:\n",
    "        os.mkdir(os.path.join(model_run_directory, config_subfolder))\n",
    "        logging.info(f\"The folder model_dir is set to be: \\n {model_dir}\")\n",
    "    except FileExistsError:\n",
    "        logging.info(f\"FILEEXISTSERROR: The folder model_dir WAS ALREADY PRESENT and is set to be: \\n {model_dir}\")\n",
    "else:\n",
    "    logging.info(f\"The folder model_dir WAS ALREADY PRESENT and is set to be: \\n {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxPj_QV43qD5"
   },
   "source": [
    "# Train Custom TF2 Object Detector\n",
    "\n",
    "* pipeline_file: defined above in writing custom training configuration\n",
    "* model_dir: the location tensorboard logs and saved model checkpoints will save to\n",
    "* num_train_steps: how long to train for\n",
    "* num_eval_steps: perform eval on validation set after this many steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE FILE: /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/deploy/efficientdet-d0/config_6/pipeline_file.config\n",
      "\n",
      "MODEL DIRECTORY: /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS/efficientdet-d0/config_6\n",
      "\n",
      "NUMBER OF STEPS: 400000\n",
      "\n",
      "NUMBER OF EVALUATION STEPS: 3000\n"
     ]
    }
   ],
   "source": [
    "print(\"PIPELINE FILE: \" + str(pipeline_file), \n",
    "      \"MODEL DIRECTORY: \" + str(model_dir), \n",
    "      \"NUMBER OF STEPS: \" + str(num_steps), \n",
    "      \"NUMBER OF EVALUATION STEPS: \" + str(num_eval_steps), \n",
    "      sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: numpy in /anaconda/envs/py38_default/lib/python3.8/site-packages (1.19.5)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.21.4-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fbprophet 0.7.1 requires cmdstanpy==0.9.5, which is not installed.\n",
      "fbprophet 0.7.1 requires setuptools-git>=1.2, which is not installed.\n",
      "tensorflow 2.6.1 requires h5py~=3.1.0, but you have h5py 2.9.0 which is incompatible.\n",
      "tensorflow 2.6.1 requires numpy~=1.19.2, but you have numpy 1.21.4 which is incompatible.\n",
      "tensorflow-gpu 2.5.0 requires grpcio~=1.34.0, but you have grpcio 1.41.1 which is incompatible.\n",
      "tensorflow-gpu 2.5.0 requires h5py~=3.1.0, but you have h5py 2.9.0 which is incompatible.\n",
      "tensorflow-gpu 2.5.0 requires numpy~=1.19.2, but you have numpy 1.21.4 which is incompatible.\n",
      "tensorflow-gpu 2.5.0 requires tensorflow-estimator<2.6.0,>=2.5.0rc0, but you have tensorflow-estimator 2.2.0 which is incompatible.\n",
      "networkx 2.6.1 requires scipy!=1.6.1,>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
      "apache-beam 2.33.0 requires numpy<1.21.0,>=1.14.3, but you have numpy 1.21.4 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.21.4\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -5py (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/py38_default/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/anaconda/envs/py38_default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQTfZChVzzpZ",
    "outputId": "48bf9506-ddd1-4de9-9629-d55602e1fea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-17 14:30:58.524391: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-17 14:30:59.005794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11567 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0001:00:00.0, compute capability: 3.7\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "I1117 14:30:59.142397 140180763812800 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: 400000\n",
      "I1117 14:30:59.146613 140180763812800 config_util.py:552] Maybe overwriting train_steps: 400000\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I1117 14:30:59.146743 140180763812800 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "I1117 14:30:59.170581 140180763812800 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
      "I1117 14:30:59.170693 140180763812800 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n",
      "I1117 14:30:59.170764 140180763812800 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 3\n",
      "I1117 14:30:59.174509 140180763812800 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I1117 14:30:59.192964 140180763812800 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I1117 14:30:59.233440 140180763812800 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I1117 14:30:59.236041 140180763812800 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I1117 14:30:59.236864 140180763812800 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I1117 14:30:59.243867 140180763812800 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I1117 14:30:59.246972 140180763812800 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I1117 14:30:59.252291 140180763812800 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1117 14:30:59.252385 140180763812800 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I1117 14:30:59.265520 140180763812800 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I1117 14:30:59.266317 140180763812800 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I1117 14:30:59.267833 140180763812800 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I1117 14:30:59.268614 140180763812800 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I1117 14:30:59.341931 140180763812800 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1117 14:30:59.342026 140180763812800 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1117 14:30:59.693709 140180763812800 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1117 14:30:59.693865 140180763812800 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1117 14:30:59.934303 140180763812800 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1117 14:30:59.934453 140180763812800 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1117 14:31:00.287684 140180763812800 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1117 14:31:00.287833 140180763812800 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1117 14:31:00.645277 140180763812800 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1117 14:31:00.645404 140180763812800 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1117 14:31:01.117656 140180763812800 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1117 14:31:01.117810 140180763812800 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I1117 14:31:01.231197 140180763812800 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I1117 14:31:01.275222 140180763812800 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W1117 14:31:01.315864 140180763812800 deprecation.py:339] From /anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading unweighted datasets: ['/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pictures/output_tfrecords/train/merged_logos.tfrecord']\n",
      "I1117 14:31:01.349594 140180763812800 dataset_builder.py:163] Reading unweighted datasets: ['/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pictures/output_tfrecords/train/merged_logos.tfrecord']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pictures/output_tfrecords/train/merged_logos.tfrecord']\n",
      "I1117 14:31:01.349800 140180763812800 dataset_builder.py:80] Reading record datasets for input file: ['/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pictures/output_tfrecords/train/merged_logos.tfrecord']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I1117 14:31:01.349889 140180763812800 dataset_builder.py:81] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W1117 14:31:01.349958 140180763812800 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "W1117 14:31:01.352504 140180763812800 deprecation.py:339] From /anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W1117 14:31:01.375789 140180763812800 deprecation.py:339] From /anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W1117 14:31:08.440877 140180763812800 deprecation.py:339] From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1117 14:31:12.599126 140180763812800 deprecation.py:339] From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "2021-11-17 14:31:15.282392: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/keras/backend.py:401: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
      "2021-11-17 14:32:19.874193: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8202\n",
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:617: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W1117 14:33:11.058253 140174811715328 deprecation.py:542] From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:617: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
      "W1117 14:33:21.189736 140174811715328 utils.py:75] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
      "W1117 14:33:34.640956 140174811715328 utils.py:75] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
      "W1117 14:33:47.578072 140174811715328 utils.py:75] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
      "W1117 14:34:01.113554 140174811715328 utils.py:75] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
      "INFO:tensorflow:Step 36100 per-step time 3.751s\n",
      "I1117 14:39:25.717518 140180763812800 model_lib_v2.py:698] Step 36100 per-step time 3.751s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.32041678,\n",
      " 'Loss/localization_loss': 0.24346168,\n",
      " 'Loss/regularization_loss': 0.071031615,\n",
      " 'Loss/total_loss': 0.63491005,\n",
      " 'learning_rate': 0.07750843}\n",
      "I1117 14:39:25.717831 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.32041678,\n",
      " 'Loss/localization_loss': 0.24346168,\n",
      " 'Loss/regularization_loss': 0.071031615,\n",
      " 'Loss/total_loss': 0.63491005,\n",
      " 'learning_rate': 0.07750843}\n",
      "INFO:tensorflow:Step 36200 per-step time 2.751s\n",
      "I1117 14:44:00.828430 140180763812800 model_lib_v2.py:698] Step 36200 per-step time 2.751s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.46249056,\n",
      " 'Loss/localization_loss': 0.21516927,\n",
      " 'Loss/regularization_loss': 0.07109011,\n",
      " 'Loss/total_loss': 0.7487499,\n",
      " 'learning_rate': 0.077493735}\n",
      "I1117 14:44:00.828714 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.46249056,\n",
      " 'Loss/localization_loss': 0.21516927,\n",
      " 'Loss/regularization_loss': 0.07109011,\n",
      " 'Loss/total_loss': 0.7487499,\n",
      " 'learning_rate': 0.077493735}\n",
      "INFO:tensorflow:Step 36300 per-step time 2.746s\n",
      "I1117 14:48:35.394602 140180763812800 model_lib_v2.py:698] Step 36300 per-step time 2.746s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.3646826,\n",
      " 'Loss/localization_loss': 0.25860122,\n",
      " 'Loss/regularization_loss': 0.0711409,\n",
      " 'Loss/total_loss': 0.69442475,\n",
      " 'learning_rate': 0.07747899}\n",
      "I1117 14:48:35.394872 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.3646826,\n",
      " 'Loss/localization_loss': 0.25860122,\n",
      " 'Loss/regularization_loss': 0.0711409,\n",
      " 'Loss/total_loss': 0.69442475,\n",
      " 'learning_rate': 0.07747899}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step 36400 per-step time 2.752s\n",
      "I1117 14:53:10.615199 140180763812800 model_lib_v2.py:698] Step 36400 per-step time 2.752s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.27686253,\n",
      " 'Loss/localization_loss': 0.14377001,\n",
      " 'Loss/regularization_loss': 0.071173504,\n",
      " 'Loss/total_loss': 0.49180606,\n",
      " 'learning_rate': 0.077464215}\n",
      "I1117 14:53:10.615471 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.27686253,\n",
      " 'Loss/localization_loss': 0.14377001,\n",
      " 'Loss/regularization_loss': 0.071173504,\n",
      " 'Loss/total_loss': 0.49180606,\n",
      " 'learning_rate': 0.077464215}\n",
      "INFO:tensorflow:Step 36500 per-step time 2.770s\n",
      "I1117 14:57:47.622115 140180763812800 model_lib_v2.py:698] Step 36500 per-step time 2.770s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.27966288,\n",
      " 'Loss/localization_loss': 0.24064836,\n",
      " 'Loss/regularization_loss': 0.07121254,\n",
      " 'Loss/total_loss': 0.59152377,\n",
      " 'learning_rate': 0.077449396}\n",
      "I1117 14:57:47.622406 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.27966288,\n",
      " 'Loss/localization_loss': 0.24064836,\n",
      " 'Loss/regularization_loss': 0.07121254,\n",
      " 'Loss/total_loss': 0.59152377,\n",
      " 'learning_rate': 0.077449396}\n",
      "INFO:tensorflow:Step 36600 per-step time 2.772s\n",
      "I1117 15:02:24.866762 140180763812800 model_lib_v2.py:698] Step 36600 per-step time 2.772s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.5360884,\n",
      " 'Loss/localization_loss': 0.2951513,\n",
      " 'Loss/regularization_loss': 0.07125686,\n",
      " 'Loss/total_loss': 0.9024966,\n",
      " 'learning_rate': 0.077434525}\n",
      "I1117 15:02:24.867041 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.5360884,\n",
      " 'Loss/localization_loss': 0.2951513,\n",
      " 'Loss/regularization_loss': 0.07125686,\n",
      " 'Loss/total_loss': 0.9024966,\n",
      " 'learning_rate': 0.077434525}\n",
      "INFO:tensorflow:Step 36700 per-step time 2.772s\n",
      "I1117 15:07:02.112400 140180763812800 model_lib_v2.py:698] Step 36700 per-step time 2.772s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.43287966,\n",
      " 'Loss/localization_loss': 0.2842208,\n",
      " 'Loss/regularization_loss': 0.07128714,\n",
      " 'Loss/total_loss': 0.7883876,\n",
      " 'learning_rate': 0.077419624}\n",
      "I1117 15:07:02.112701 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.43287966,\n",
      " 'Loss/localization_loss': 0.2842208,\n",
      " 'Loss/regularization_loss': 0.07128714,\n",
      " 'Loss/total_loss': 0.7883876,\n",
      " 'learning_rate': 0.077419624}\n",
      "INFO:tensorflow:Step 36800 per-step time 2.766s\n",
      "I1117 15:11:38.724235 140180763812800 model_lib_v2.py:698] Step 36800 per-step time 2.766s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.38933215,\n",
      " 'Loss/localization_loss': 0.24230315,\n",
      " 'Loss/regularization_loss': 0.07131364,\n",
      " 'Loss/total_loss': 0.7029489,\n",
      " 'learning_rate': 0.07740468}\n",
      "I1117 15:11:38.724531 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.38933215,\n",
      " 'Loss/localization_loss': 0.24230315,\n",
      " 'Loss/regularization_loss': 0.07131364,\n",
      " 'Loss/total_loss': 0.7029489,\n",
      " 'learning_rate': 0.07740468}\n",
      "INFO:tensorflow:Step 36900 per-step time 2.756s\n",
      "I1117 15:16:14.358660 140180763812800 model_lib_v2.py:698] Step 36900 per-step time 2.756s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.41207024,\n",
      " 'Loss/localization_loss': 0.1988093,\n",
      " 'Loss/regularization_loss': 0.071331725,\n",
      " 'Loss/total_loss': 0.6822113,\n",
      " 'learning_rate': 0.077389695}\n",
      "I1117 15:16:14.358935 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.41207024,\n",
      " 'Loss/localization_loss': 0.1988093,\n",
      " 'Loss/regularization_loss': 0.071331725,\n",
      " 'Loss/total_loss': 0.6822113,\n",
      " 'learning_rate': 0.077389695}\n",
      "INFO:tensorflow:Step 37000 per-step time 2.771s\n",
      "I1117 15:20:51.472746 140180763812800 model_lib_v2.py:698] Step 37000 per-step time 2.771s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.3516771,\n",
      " 'Loss/localization_loss': 0.14123416,\n",
      " 'Loss/regularization_loss': 0.07137345,\n",
      " 'Loss/total_loss': 0.5642847,\n",
      " 'learning_rate': 0.07737466}\n",
      "I1117 15:20:51.473017 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.3516771,\n",
      " 'Loss/localization_loss': 0.14123416,\n",
      " 'Loss/regularization_loss': 0.07137345,\n",
      " 'Loss/total_loss': 0.5642847,\n",
      " 'learning_rate': 0.07737466}\n",
      "INFO:tensorflow:Step 37100 per-step time 2.761s\n",
      "I1117 15:25:27.599541 140180763812800 model_lib_v2.py:698] Step 37100 per-step time 2.761s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.33689108,\n",
      " 'Loss/localization_loss': 0.23381624,\n",
      " 'Loss/regularization_loss': 0.0713642,\n",
      " 'Loss/total_loss': 0.6420715,\n",
      " 'learning_rate': 0.07735959}\n",
      "I1117 15:25:27.599817 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.33689108,\n",
      " 'Loss/localization_loss': 0.23381624,\n",
      " 'Loss/regularization_loss': 0.0713642,\n",
      " 'Loss/total_loss': 0.6420715,\n",
      " 'learning_rate': 0.07735959}\n",
      "INFO:tensorflow:Step 37200 per-step time 2.769s\n",
      "I1117 15:30:04.530689 140180763812800 model_lib_v2.py:698] Step 37200 per-step time 2.769s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.21857563,\n",
      " 'Loss/localization_loss': 0.10091971,\n",
      " 'Loss/regularization_loss': 0.07134981,\n",
      " 'Loss/total_loss': 0.39084512,\n",
      " 'learning_rate': 0.07734448}\n",
      "I1117 15:30:04.530956 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.21857563,\n",
      " 'Loss/localization_loss': 0.10091971,\n",
      " 'Loss/regularization_loss': 0.07134981,\n",
      " 'Loss/total_loss': 0.39084512,\n",
      " 'learning_rate': 0.07734448}\n",
      "INFO:tensorflow:Step 37300 per-step time 2.778s\n",
      "I1117 15:34:42.281433 140180763812800 model_lib_v2.py:698] Step 37300 per-step time 2.778s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.28048012,\n",
      " 'Loss/localization_loss': 0.17687318,\n",
      " 'Loss/regularization_loss': 0.07132694,\n",
      " 'Loss/total_loss': 0.5286802,\n",
      " 'learning_rate': 0.07732932}\n",
      "I1117 15:34:42.281724 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.28048012,\n",
      " 'Loss/localization_loss': 0.17687318,\n",
      " 'Loss/regularization_loss': 0.07132694,\n",
      " 'Loss/total_loss': 0.5286802,\n",
      " 'learning_rate': 0.07732932}\n",
      "INFO:tensorflow:Step 37400 per-step time 2.771s\n",
      "I1117 15:39:19.332344 140180763812800 model_lib_v2.py:698] Step 37400 per-step time 2.771s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.24322124,\n",
      " 'Loss/localization_loss': 0.14949593,\n",
      " 'Loss/regularization_loss': 0.07132029,\n",
      " 'Loss/total_loss': 0.46403745,\n",
      " 'learning_rate': 0.07731412}\n",
      "I1117 15:39:19.332623 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.24322124,\n",
      " 'Loss/localization_loss': 0.14949593,\n",
      " 'Loss/regularization_loss': 0.07132029,\n",
      " 'Loss/total_loss': 0.46403745,\n",
      " 'learning_rate': 0.07731412}\n",
      "INFO:tensorflow:Step 37500 per-step time 2.763s\n",
      "I1117 15:43:55.665154 140180763812800 model_lib_v2.py:698] Step 37500 per-step time 2.763s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.3065466,\n",
      " 'Loss/localization_loss': 0.16103607,\n",
      " 'Loss/regularization_loss': 0.07129648,\n",
      " 'Loss/total_loss': 0.53887916,\n",
      " 'learning_rate': 0.07729889}\n",
      "I1117 15:43:55.665422 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.3065466,\n",
      " 'Loss/localization_loss': 0.16103607,\n",
      " 'Loss/regularization_loss': 0.07129648,\n",
      " 'Loss/total_loss': 0.53887916,\n",
      " 'learning_rate': 0.07729889}\n",
      "INFO:tensorflow:Step 37600 per-step time 2.764s\n",
      "I1117 15:48:32.089168 140180763812800 model_lib_v2.py:698] Step 37600 per-step time 2.764s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.36841655,\n",
      " 'Loss/localization_loss': 0.24424711,\n",
      " 'Loss/regularization_loss': 0.07129866,\n",
      " 'Loss/total_loss': 0.68396235,\n",
      " 'learning_rate': 0.077283606}\n",
      "I1117 15:48:32.089448 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.36841655,\n",
      " 'Loss/localization_loss': 0.24424711,\n",
      " 'Loss/regularization_loss': 0.07129866,\n",
      " 'Loss/total_loss': 0.68396235,\n",
      " 'learning_rate': 0.077283606}\n",
      "INFO:tensorflow:Step 37700 per-step time 2.760s\n",
      "I1117 15:53:08.119393 140180763812800 model_lib_v2.py:698] Step 37700 per-step time 2.760s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.4503438,\n",
      " 'Loss/localization_loss': 0.22359608,\n",
      " 'Loss/regularization_loss': 0.07127366,\n",
      " 'Loss/total_loss': 0.74521357,\n",
      " 'learning_rate': 0.07726829}\n",
      "I1117 15:53:08.119667 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.4503438,\n",
      " 'Loss/localization_loss': 0.22359608,\n",
      " 'Loss/regularization_loss': 0.07127366,\n",
      " 'Loss/total_loss': 0.74521357,\n",
      " 'learning_rate': 0.07726829}\n",
      "INFO:tensorflow:Step 37800 per-step time 2.775s\n",
      "I1117 15:57:45.631045 140180763812800 model_lib_v2.py:698] Step 37800 per-step time 2.775s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.16260277,\n",
      " 'Loss/localization_loss': 0.067660615,\n",
      " 'Loss/regularization_loss': 0.07124032,\n",
      " 'Loss/total_loss': 0.30150372,\n",
      " 'learning_rate': 0.07725292}\n",
      "I1117 15:57:45.631332 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.16260277,\n",
      " 'Loss/localization_loss': 0.067660615,\n",
      " 'Loss/regularization_loss': 0.07124032,\n",
      " 'Loss/total_loss': 0.30150372,\n",
      " 'learning_rate': 0.07725292}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step 37900 per-step time 2.767s\n",
      "I1117 16:02:22.294371 140180763812800 model_lib_v2.py:698] Step 37900 per-step time 2.767s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.25367445,\n",
      " 'Loss/localization_loss': 0.17999874,\n",
      " 'Loss/regularization_loss': 0.071216345,\n",
      " 'Loss/total_loss': 0.50488955,\n",
      " 'learning_rate': 0.07723752}\n",
      "I1117 16:02:22.294661 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.25367445,\n",
      " 'Loss/localization_loss': 0.17999874,\n",
      " 'Loss/regularization_loss': 0.071216345,\n",
      " 'Loss/total_loss': 0.50488955,\n",
      " 'learning_rate': 0.07723752}\n",
      "INFO:tensorflow:Step 38000 per-step time 2.775s\n",
      "I1117 16:06:59.779243 140180763812800 model_lib_v2.py:698] Step 38000 per-step time 2.775s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.30014488,\n",
      " 'Loss/localization_loss': 0.13751608,\n",
      " 'Loss/regularization_loss': 0.071184434,\n",
      " 'Loss/total_loss': 0.5088454,\n",
      " 'learning_rate': 0.07722207}\n",
      "I1117 16:06:59.779517 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.30014488,\n",
      " 'Loss/localization_loss': 0.13751608,\n",
      " 'Loss/regularization_loss': 0.071184434,\n",
      " 'Loss/total_loss': 0.5088454,\n",
      " 'learning_rate': 0.07722207}\n",
      "INFO:tensorflow:Step 38100 per-step time 2.775s\n",
      "I1117 16:11:37.248215 140180763812800 model_lib_v2.py:698] Step 38100 per-step time 2.775s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.20151578,\n",
      " 'Loss/localization_loss': 0.11061978,\n",
      " 'Loss/regularization_loss': 0.07116402,\n",
      " 'Loss/total_loss': 0.3832996,\n",
      " 'learning_rate': 0.07720659}\n",
      "I1117 16:11:37.248516 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.20151578,\n",
      " 'Loss/localization_loss': 0.11061978,\n",
      " 'Loss/regularization_loss': 0.07116402,\n",
      " 'Loss/total_loss': 0.3832996,\n",
      " 'learning_rate': 0.07720659}\n",
      "INFO:tensorflow:Step 38200 per-step time 2.761s\n",
      "I1117 16:16:13.394637 140180763812800 model_lib_v2.py:698] Step 38200 per-step time 2.761s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.38128698,\n",
      " 'Loss/localization_loss': 0.18671536,\n",
      " 'Loss/regularization_loss': 0.07113085,\n",
      " 'Loss/total_loss': 0.6391332,\n",
      " 'learning_rate': 0.077191055}\n",
      "I1117 16:16:13.394937 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.38128698,\n",
      " 'Loss/localization_loss': 0.18671536,\n",
      " 'Loss/regularization_loss': 0.07113085,\n",
      " 'Loss/total_loss': 0.6391332,\n",
      " 'learning_rate': 0.077191055}\n",
      "INFO:tensorflow:Step 38300 per-step time 2.767s\n",
      "I1117 16:20:50.058730 140180763812800 model_lib_v2.py:698] Step 38300 per-step time 2.767s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.25385267,\n",
      " 'Loss/localization_loss': 0.107538775,\n",
      " 'Loss/regularization_loss': 0.07107231,\n",
      " 'Loss/total_loss': 0.43246377,\n",
      " 'learning_rate': 0.07717548}\n",
      "I1117 16:20:50.058994 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.25385267,\n",
      " 'Loss/localization_loss': 0.107538775,\n",
      " 'Loss/regularization_loss': 0.07107231,\n",
      " 'Loss/total_loss': 0.43246377,\n",
      " 'learning_rate': 0.07717548}\n",
      "INFO:tensorflow:Step 38400 per-step time 2.769s\n",
      "I1117 16:25:26.956013 140180763812800 model_lib_v2.py:698] Step 38400 per-step time 2.769s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.15873678,\n",
      " 'Loss/localization_loss': 0.07192375,\n",
      " 'Loss/regularization_loss': 0.071009144,\n",
      " 'Loss/total_loss': 0.30166966,\n",
      " 'learning_rate': 0.077159874}\n",
      "I1117 16:25:26.956285 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.15873678,\n",
      " 'Loss/localization_loss': 0.07192375,\n",
      " 'Loss/regularization_loss': 0.071009144,\n",
      " 'Loss/total_loss': 0.30166966,\n",
      " 'learning_rate': 0.077159874}\n",
      "INFO:tensorflow:Step 38500 per-step time 2.766s\n",
      "I1117 16:30:03.544922 140180763812800 model_lib_v2.py:698] Step 38500 per-step time 2.766s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.15190879,\n",
      " 'Loss/localization_loss': 0.054689426,\n",
      " 'Loss/regularization_loss': 0.0709193,\n",
      " 'Loss/total_loss': 0.27751753,\n",
      " 'learning_rate': 0.07714422}\n",
      "I1117 16:30:03.545190 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.15190879,\n",
      " 'Loss/localization_loss': 0.054689426,\n",
      " 'Loss/regularization_loss': 0.0709193,\n",
      " 'Loss/total_loss': 0.27751753,\n",
      " 'learning_rate': 0.07714422}\n",
      "INFO:tensorflow:Step 38600 per-step time 2.763s\n",
      "I1117 16:34:39.819620 140180763812800 model_lib_v2.py:698] Step 38600 per-step time 2.763s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.14825052,\n",
      " 'Loss/localization_loss': 0.056775346,\n",
      " 'Loss/regularization_loss': 0.070837185,\n",
      " 'Loss/total_loss': 0.27586305,\n",
      " 'learning_rate': 0.07712853}\n",
      "I1117 16:34:39.819902 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.14825052,\n",
      " 'Loss/localization_loss': 0.056775346,\n",
      " 'Loss/regularization_loss': 0.070837185,\n",
      " 'Loss/total_loss': 0.27586305,\n",
      " 'learning_rate': 0.07712853}\n",
      "INFO:tensorflow:Step 38700 per-step time 2.769s\n",
      "I1117 16:39:16.696753 140180763812800 model_lib_v2.py:698] Step 38700 per-step time 2.769s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.10417034,\n",
      " 'Loss/localization_loss': 0.026262395,\n",
      " 'Loss/regularization_loss': 0.07072827,\n",
      " 'Loss/total_loss': 0.201161,\n",
      " 'learning_rate': 0.077112794}\n",
      "I1117 16:39:16.697038 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.10417034,\n",
      " 'Loss/localization_loss': 0.026262395,\n",
      " 'Loss/regularization_loss': 0.07072827,\n",
      " 'Loss/total_loss': 0.201161,\n",
      " 'learning_rate': 0.077112794}\n",
      "INFO:tensorflow:Step 38800 per-step time 2.765s\n",
      "I1117 16:43:53.193328 140180763812800 model_lib_v2.py:698] Step 38800 per-step time 2.765s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.14758644,\n",
      " 'Loss/localization_loss': 0.046324532,\n",
      " 'Loss/regularization_loss': 0.07061178,\n",
      " 'Loss/total_loss': 0.26452273,\n",
      " 'learning_rate': 0.07709701}\n",
      "I1117 16:43:53.193599 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.14758644,\n",
      " 'Loss/localization_loss': 0.046324532,\n",
      " 'Loss/regularization_loss': 0.07061178,\n",
      " 'Loss/total_loss': 0.26452273,\n",
      " 'learning_rate': 0.07709701}\n",
      "INFO:tensorflow:Step 38900 per-step time 2.762s\n",
      "I1117 16:48:29.437074 140180763812800 model_lib_v2.py:698] Step 38900 per-step time 2.762s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.23951825,\n",
      " 'Loss/localization_loss': 0.07845783,\n",
      " 'Loss/regularization_loss': 0.070492424,\n",
      " 'Loss/total_loss': 0.3884685,\n",
      " 'learning_rate': 0.077081196}\n",
      "I1117 16:48:29.437357 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.23951825,\n",
      " 'Loss/localization_loss': 0.07845783,\n",
      " 'Loss/regularization_loss': 0.070492424,\n",
      " 'Loss/total_loss': 0.3884685,\n",
      " 'learning_rate': 0.077081196}\n",
      "INFO:tensorflow:Step 39000 per-step time 2.765s\n",
      "I1117 16:53:05.949444 140180763812800 model_lib_v2.py:698] Step 39000 per-step time 2.765s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.16339532,\n",
      " 'Loss/localization_loss': 0.040679533,\n",
      " 'Loss/regularization_loss': 0.070362575,\n",
      " 'Loss/total_loss': 0.27443743,\n",
      " 'learning_rate': 0.07706533}\n",
      "I1117 16:53:05.949725 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.16339532,\n",
      " 'Loss/localization_loss': 0.040679533,\n",
      " 'Loss/regularization_loss': 0.070362575,\n",
      " 'Loss/total_loss': 0.27443743,\n",
      " 'learning_rate': 0.07706533}\n",
      "INFO:tensorflow:Step 39100 per-step time 2.780s\n",
      "I1117 16:57:43.933529 140180763812800 model_lib_v2.py:698] Step 39100 per-step time 2.780s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2554464,\n",
      " 'Loss/localization_loss': 0.07524934,\n",
      " 'Loss/regularization_loss': 0.0702318,\n",
      " 'Loss/total_loss': 0.40092754,\n",
      " 'learning_rate': 0.077049434}\n",
      "I1117 16:57:43.933804 140180763812800 model_lib_v2.py:701] {'Loss/classification_loss': 0.2554464,\n",
      " 'Loss/localization_loss': 0.07524934,\n",
      " 'Loss/regularization_loss': 0.0702318,\n",
      " 'Loss/total_loss': 0.40092754,\n",
      " 'learning_rate': 0.077049434}\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -u /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/object_detection/model_main_tf2.py \\\n",
    "    --pipeline_config_path={pipeline_file} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps={num_steps} \\\n",
    "    --sample_1_of_n_eval_examples=1 \\\n",
    "    --num_eval_steps={num_eval_steps} 2>&1 | sed -e \"/nan/q9\";echo $? > exitcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "9KNv1N_hUibE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
      "W1117 16:57:55.216971 139901781500864 model_lib_v2.py:1081] Forced number of epochs for all eval validations to be 1.\n",
      "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: None\n",
      "I1117 16:57:55.217181 139901781500864 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: None\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I1117 16:57:55.217309 139901781500864 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
      "I1117 16:57:55.217396 139901781500864 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
      "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "W1117 16:57:55.217512 139901781500864 model_lib_v2.py:1099] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "2021-11-17 16:57:55.221450: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-17 16:57:55.752311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11567 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0001:00:00.0, compute capability: 3.7\n",
      "I1117 16:57:55.897058 139901781500864 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
      "I1117 16:57:55.897253 139901781500864 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n",
      "I1117 16:57:55.897354 139901781500864 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 3\n",
      "I1117 16:57:55.901373 139901781500864 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1117 16:57:55.924342 139901781500864 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1117 16:57:55.924443 139901781500864 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1117 16:57:55.989363 139901781500864 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1117 16:57:55.989531 139901781500864 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1117 16:57:56.281478 139901781500864 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1117 16:57:56.281658 139901781500864 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1117 16:57:56.441883 139901781500864 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1117 16:57:56.442046 139901781500864 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1117 16:57:56.687536 139901781500864 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1117 16:57:56.687701 139901781500864 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1117 16:57:56.922777 139901781500864 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1117 16:57:56.922947 139901781500864 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1117 16:57:57.231817 139901781500864 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1117 16:57:57.231977 139901781500864 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I1117 16:57:57.303842 139901781500864 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I1117 16:57:57.334383 139901781500864 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "INFO:tensorflow:Reading unweighted datasets: ['/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pictures/output_tfrecords/valid/merged_logos.tfrecord']\n",
      "I1117 16:57:57.382764 139901781500864 dataset_builder.py:163] Reading unweighted datasets: ['/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pictures/output_tfrecords/valid/merged_logos.tfrecord']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pictures/output_tfrecords/valid/merged_logos.tfrecord']\n",
      "I1117 16:57:57.383189 139901781500864 dataset_builder.py:80] Reading record datasets for input file: ['/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/pictures/output_tfrecords/valid/merged_logos.tfrecord']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I1117 16:57:57.383296 139901781500864 dataset_builder.py:81] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W1117 16:57:57.383371 139901781500864 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "W1117 16:57:57.384662 139901781500864 deprecation.py:339] From /anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W1117 16:57:57.408247 139901781500864 deprecation.py:339] From /anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W1117 16:58:01.523155 139901781500864 deprecation.py:339] From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1117 16:58:02.729033 139901781500864 deprecation.py:339] From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Waiting for new checkpoint at /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS/efficientdet-d0/config_6\n",
      "I1117 16:58:05.423066 139901781500864 checkpoint_utils.py:140] Waiting for new checkpoint at /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS/efficientdet-d0/config_6\n",
      "INFO:tensorflow:Found new checkpoint at /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS/efficientdet-d0/config_6/ckpt-40\n",
      "I1117 16:58:05.424209 139901781500864 checkpoint_utils.py:149] Found new checkpoint at /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS/efficientdet-d0/config_6/ckpt-40\n",
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/keras/backend.py:401: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
      "2021-11-17 16:58:07.460631: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-11-17 16:59:47.232051: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8202\n",
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/eval_util.py:929: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1117 16:59:49.277916 139901781500864 deprecation.py:339] From /anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/eval_util.py:929: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Finished eval step 0\n",
      "I1117 16:59:49.397564 139901781500864 model_lib_v2.py:958] Finished eval step 0\n",
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:464: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "W1117 16:59:49.520860 139901781500864 deprecation.py:339] From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:464: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "INFO:tensorflow:Finished eval step 100\n",
      "I1117 17:01:42.034127 139901781500864 model_lib_v2.py:958] Finished eval step 100\n",
      "INFO:tensorflow:Finished eval step 200\n",
      "I1117 17:03:08.329939 139901781500864 model_lib_v2.py:958] Finished eval step 200\n",
      "INFO:tensorflow:Finished eval step 300\n",
      "I1117 17:04:34.361888 139901781500864 model_lib_v2.py:958] Finished eval step 300\n",
      "INFO:tensorflow:Finished eval step 400\n",
      "I1117 17:06:00.385133 139901781500864 model_lib_v2.py:958] Finished eval step 400\n",
      "INFO:tensorflow:Finished eval step 500\n",
      "I1117 17:07:27.014330 139901781500864 model_lib_v2.py:958] Finished eval step 500\n",
      "INFO:tensorflow:Performing evaluation on 9008 images.\n",
      "I1117 17:08:20.101342 139901781500864 coco_evaluation.py:293] Performing evaluation on 9008 images.\n",
      "creating index...\n",
      "index created!\n",
      "INFO:tensorflow:Loading and preparing annotation results...\n",
      "I1117 17:08:20.120430 139901781500864 coco_tools.py:116] Loading and preparing annotation results...\n",
      "INFO:tensorflow:DONE (t=0.46s)\n",
      "I1117 17:08:20.585524 139901781500864 coco_tools.py:138] DONE (t=0.46s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=38.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=19.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.622\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.860\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.745\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.360\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.690\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.639\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.627\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.689\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.693\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.465\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.757\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.707\n",
      "INFO:tensorflow:Eval metrics at step 39000\n",
      "I1117 17:09:18.978051 139901781500864 model_lib_v2.py:1007] Eval metrics at step 39000\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP: 0.621573\n",
      "I1117 17:09:18.981961 139901781500864 model_lib_v2.py:1010] \t+ DetectionBoxes_Precision/mAP: 0.621573\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP@.50IOU: 0.859954\n",
      "I1117 17:09:18.983027 139901781500864 model_lib_v2.py:1010] \t+ DetectionBoxes_Precision/mAP@.50IOU: 0.859954\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP@.75IOU: 0.745017\n",
      "I1117 17:09:18.984041 139901781500864 model_lib_v2.py:1010] \t+ DetectionBoxes_Precision/mAP@.75IOU: 0.745017\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP (small): 0.359989\n",
      "I1117 17:09:18.985052 139901781500864 model_lib_v2.py:1010] \t+ DetectionBoxes_Precision/mAP (small): 0.359989\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP (medium): 0.690054\n",
      "I1117 17:09:18.986064 139901781500864 model_lib_v2.py:1010] \t+ DetectionBoxes_Precision/mAP (medium): 0.690054\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP (large): 0.639244\n",
      "I1117 17:09:18.987061 139901781500864 model_lib_v2.py:1010] \t+ DetectionBoxes_Precision/mAP (large): 0.639244\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@1: 0.627302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1117 17:09:18.988135 139901781500864 model_lib_v2.py:1010] \t+ DetectionBoxes_Recall/AR@1: 0.627302\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@10: 0.688869\n",
      "I1117 17:09:18.989261 139901781500864 model_lib_v2.py:1010] \t+ DetectionBoxes_Recall/AR@10: 0.688869\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100: 0.692615\n",
      "I1117 17:09:18.990267 139901781500864 model_lib_v2.py:1010] \t+ DetectionBoxes_Recall/AR@100: 0.692615\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100 (small): 0.464991\n",
      "I1117 17:09:18.991269 139901781500864 model_lib_v2.py:1010] \t+ DetectionBoxes_Recall/AR@100 (small): 0.464991\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100 (medium): 0.757221\n",
      "I1117 17:09:18.992248 139901781500864 model_lib_v2.py:1010] \t+ DetectionBoxes_Recall/AR@100 (medium): 0.757221\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100 (large): 0.707454\n",
      "I1117 17:09:18.993275 139901781500864 model_lib_v2.py:1010] \t+ DetectionBoxes_Recall/AR@100 (large): 0.707454\n",
      "INFO:tensorflow:\t+ Loss/localization_loss: 0.081886\n",
      "I1117 17:09:18.994071 139901781500864 model_lib_v2.py:1010] \t+ Loss/localization_loss: 0.081886\n",
      "INFO:tensorflow:\t+ Loss/classification_loss: 0.221574\n",
      "I1117 17:09:18.994887 139901781500864 model_lib_v2.py:1010] \t+ Loss/classification_loss: 0.221574\n",
      "INFO:tensorflow:\t+ Loss/regularization_loss: 0.070361\n",
      "I1117 17:09:18.995735 139901781500864 model_lib_v2.py:1010] \t+ Loss/regularization_loss: 0.070361\n",
      "INFO:tensorflow:\t+ Loss/total_loss: 0.373821\n",
      "I1117 17:09:18.996587 139901781500864 model_lib_v2.py:1010] \t+ Loss/total_loss: 0.373821\n",
      "INFO:tensorflow:Waiting for new checkpoint at /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS/efficientdet-d0/config_6\n",
      "I1117 17:09:20.654663 139901781500864 checkpoint_utils.py:140] Waiting for new checkpoint at /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS/efficientdet-d0/config_6\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/object_detection/model_main_tf2.py\", line 115, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/site-packages/absl/app.py\", line 303, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/site-packages/absl/app.py\", line 251, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/object_detection/model_main_tf2.py\", line 82, in main\n",
      "    model_lib_v2.eval_continuously(\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/site-packages/object_detection/model_lib_v2.py\", line 1128, in eval_continuously\n",
      "    for latest_checkpoint in tf.train.checkpoints_iterator(\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 198, in checkpoints_iterator\n",
      "    new_checkpoint_path = wait_for_new_checkpoint(\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 147, in wait_for_new_checkpoint\n",
      "    time.sleep(seconds_to_sleep)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#run model evaluation to obtain performance metrics\n",
    "\n",
    "!python /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/object_detection/model_main_tf2.py \\\n",
    "    --pipeline_config_path={pipeline_file} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --checkpoint_dir={model_dir} \\\n",
    "\n",
    "#Not yet implemented for EfficientDet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Tensorboard\n",
    "Remember to pass the path into the magic command as follows https://stackoverflow.com/questions/14409167/how-to-pass-a-variable-to-magic-%C2%B4run%C2%B4-function-in-ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_training_directory = os.path.join(model_dir, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Vk2146Ogil3"
   },
   "source": [
    "## Exporting a Trained Inference Graph\n",
    "Still to come for TF2 models, we will be updating this Colab notebook accordingly as the functionality is added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqaZ4v-vIuDl",
    "outputId": "050f887a-7594-4359-d084-e0ac99865e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                   ckpt-38.data-00000-of-00001\r\n",
      "ckpt-34.data-00000-of-00001  ckpt-38.index\r\n",
      "ckpt-34.index                ckpt-39.data-00000-of-00001\r\n",
      "ckpt-35.data-00000-of-00001  ckpt-39.index\r\n",
      "ckpt-35.index                ckpt-40.data-00000-of-00001\r\n",
      "ckpt-36.data-00000-of-00001  ckpt-40.index\r\n",
      "ckpt-36.index                \u001b[0m\u001b[01;34meval\u001b[0m/\r\n",
      "ckpt-37.data-00000-of-00001  \u001b[01;34mtrain\u001b[0m/\r\n",
      "ckpt-37.index\r\n"
     ]
    }
   ],
   "source": [
    "#see where our model saved weights\n",
    "%ls $model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training\n"
     ]
    }
   ],
   "source": [
    "%cd /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS/efficientdet-d0/config_6'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnSEZIzl4M10",
    "outputId": "22c6bedb-294a-414a-93bf-b14a76ff481c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The directory FINE_TUNED_MODEL is already present, files will be stored there\n",
      "INFO:root:The folder model_run_directory WAS ALREADY PRESENT and is set to be: \n",
      " /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/FINE_TUNED_MODEL/efficientdet-d0\n",
      "INFO:root:The folder output_directory is set to be: \n",
      " /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/FINE_TUNED_MODEL/efficientdet-d0/config_6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/TENSOR_RESULTS/efficientdet-d0/config_6\n"
     ]
    }
   ],
   "source": [
    "#run conversion script\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "fine_tuned_directory = '/home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/FINE_TUNED_MODEL'\n",
    "\n",
    "if \"FINE_TUNED_MODEL\" not in os.listdir(os.getcwd()):\n",
    "    os.mkdir(fine_tuned_directory)\n",
    "    logging.info(\"Creating the directory TENSOR_RESULTS because it did not exist\") \n",
    "else:\n",
    "    logging.info(\"The directory FINE_TUNED_MODEL is already present, files will be stored there\")\n",
    "    \n",
    "model_fine_tuned_directory = os.path.join(fine_tuned_directory, chosen_model)\n",
    "\n",
    "if chosen_model not in os.listdir(fine_tuned_directory):\n",
    "    try:\n",
    "        os.mkdir(model_fine_tuned_directory)\n",
    "        logging.info(f\"The folder model_fine_tuned_directory is set to be: \\n {model_fine_tuned_directory}\")\n",
    "    except FileExistsError:\n",
    "        logging.info(f\"FILEEXISTSERROR: The folder model_fine_tuned_directory is set to be: \\n {model_fine_tuned_directory}\")\n",
    "else:\n",
    "    logging.info(f\"The folder model_run_directory WAS ALREADY PRESENT and is set to be: \\n {model_fine_tuned_directory}\")\n",
    "\n",
    "output_directory = os.path.join(model_fine_tuned_directory, config_subfolder.split(\"/\")[-1])\n",
    "\n",
    "if config_subfolder.split(\"/\")[-1] not in os.listdir(model_fine_tuned_directory):\n",
    "    try:\n",
    "        os.mkdir(output_directory)\n",
    "        logging.info(f\"The folder output_directory is set to be: \\n {output_directory}\")\n",
    "    except FileExistsError:\n",
    "        logging.info(f\"FILEEXISTSERROR: The folder output_directory WAS ALREADY PRESENT and is set to be: \\n {output_directory}\")\n",
    "\n",
    "#place the model weights you would like to export here\n",
    "last_model_path = model_dir\n",
    "print(last_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-17 17:11:08.043315: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-17 17:11:08.544212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11567 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0001:00:00.0, compute capability: 3.7\n",
      "I1117 17:11:08.682923 140082794759104 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
      "I1117 17:11:08.683123 140082794759104 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n",
      "I1117 17:11:08.683199 140082794759104 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 3\n",
      "I1117 17:11:08.686735 140082794759104 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1117 17:11:08.708136 140082794759104 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1117 17:11:08.708236 140082794759104 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1117 17:11:08.768943 140082794759104 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1117 17:11:08.769098 140082794759104 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1117 17:11:08.922102 140082794759104 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1117 17:11:08.922236 140082794759104 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1117 17:11:09.072509 140082794759104 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1117 17:11:09.072633 140082794759104 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1117 17:11:09.299019 140082794759104 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1117 17:11:09.299196 140082794759104 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1117 17:11:09.528554 140082794759104 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1117 17:11:09.528703 140082794759104 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1117 17:11:09.827891 140082794759104 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1117 17:11:09.828068 140082794759104 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I1117 17:11:09.898249 140082794759104 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I1117 17:11:09.929007 140082794759104 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "WARNING:tensorflow:From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "W1117 17:11:11.856000 140082794759104 deprecation.py:611] From /anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f6720332d90>, because it is not built.\n",
      "W1117 17:11:31.384112 140082794759104 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f6720332d90>, because it is not built.\n",
      "2021-11-17 17:11:58.103090: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "W1117 17:12:37.972673 140082794759104 save.py:249] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxPredictor_layer_call_fn while saving (showing 5 of 795). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/FINE_TUNED_MODEL/efficientdet-d0/config_6/saved_model/assets\n",
      "I1117 17:12:48.910276 140082794759104 builder_impl.py:780] Assets written to: /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/FINE_TUNED_MODEL/efficientdet-d0/config_6/saved_model/assets\n",
      "INFO:tensorflow:Writing pipeline config file to /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/FINE_TUNED_MODEL/efficientdet-d0/config_6/pipeline.config\n",
      "I1117 17:12:51.034945 140082794759104 config_util.py:253] Writing pipeline config file to /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/FINE_TUNED_MODEL/efficientdet-d0/config_6/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "!python /home/labuser/LogoDet/LogoDetection_DSBAProject/training_process/training/models/research/object_detection/exporter_main_v2.py \\\n",
    "    --trained_checkpoint_dir {last_model_path} \\\n",
    "    --output_directory {output_directory} \\\n",
    "    --pipeline_config_path {pipeline_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_directory = os.path.join(output_directory, \"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsE_uVjlsz3u",
    "outputId": "eb23557c-9456-43c3-a577-ea4b7ef1522b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34massets\u001b[0m/  saved_model.pb  \u001b[01;34mvariables\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls $saved_model_directory"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Roboflow-TensorFlow2-Object-Detection.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "40a8aae49d8ead891f177a9aa92f4d86d5628cb1b4814a86ede39a8e1ef554b8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
