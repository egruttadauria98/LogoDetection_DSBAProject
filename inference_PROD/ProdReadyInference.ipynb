{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production-Ready Inference\n",
    "The production-ready inference Jupyter Notebook is meant to allow a user-friendly inference that can be run in local, without the need to struggle for understanding deeply what is going on under the hood of our model and trying to find a way out from our hierarchy!\n",
    "\n",
    "Before running this notebook, make sure that you have all of these things in place:\n",
    "1. You have inserted the images for which you want your annotations to be predicted inside the **INFERENCE_DIR** folder.\n",
    "2. You have added a file named **annotations.csv** to the **ANNOTATIONS&PREDICTIONS** folder, if you want your ground-truth annotations to be compared to the predictions made by our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "  \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "  Puts image into numpy array to feed into tensorflow graph.\n",
    "  Note that by convention we put it into a numpy array with shape\n",
    "  (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "  Args:\n",
    "    path: the file path to the image\n",
    "\n",
    "  Returns:\n",
    "    uint8 numpy array with shape (img_height, img_width, 3)\n",
    "  \"\"\"\n",
    "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "  image = Image.open(BytesIO(img_data))\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: c:/Users/rebec/Documents/GitHub/ObjectRecognition/LogoDetection_DSBAProject and the fine-tuned model is contained in c:/Users/rebec/Documents/GitHub/ObjectRecognition/LogoDetection_DSBAProject/inference_PROD/model_folder/FINE_TUNED_MODEL/bestmodel\n"
     ]
    }
   ],
   "source": [
    "model = \"model_folder\"\n",
    "config = \"bestmodel\"\n",
    "\n",
    "current_working_directory = \"/\".join(os.getcwd().split(\"\\\\\")[:-1])\n",
    "output_directory = f'{current_working_directory}/inference_PROD/{model}/FINE_TUNED_MODEL/{config}'\n",
    "\n",
    "print(f\"The current working directory is: {current_working_directory} and the fine-tuned model is contained in {output_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the inference over your images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('c:/Users/rebec/Documents/GitHub/ObjectRecognition/LogoDetection_DSBAProject/inference_PROD/model_folder/FINE_TUNED_MODEL/bestmodel/checkpoint/ckpt-0.index')]\n",
      "c:/Users/rebec/Documents/GitHub/ObjectRecognition/LogoDetection_DSBAProject/inference_PROD/model_folder/deploy/config_6/pipeline_file.config\n",
      "<object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000002432C0C0E20>\n",
      "[WindowsPath('c:/Users/rebec/Documents/GitHub/ObjectRecognition/LogoDetection_DSBAProject/inference_PROD/model_folder/FINE_TUNED_MODEL/bestmodel/checkpoint/ckpt-0.index')]\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "filenames = list(pathlib.Path(os.path.join(output_directory,\"checkpoint\")).glob('*.index'))\n",
    "filenames.sort()\n",
    "print(filenames)\n",
    "\n",
    "#recover our saved model\n",
    "pipeline_file = f\"{current_working_directory}/inference_PROD/{model}/deploy/config_6/pipeline_file.config\"\n",
    "print(pipeline_file)\n",
    "pipeline_config = pipeline_file\n",
    "\n",
    "#generally you want to put the last ckpt from training in here\n",
    "model_dir = str(filenames).replace('.index','')\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "#print(model_config)\n",
    "detection_model = model_builder.build(\n",
    "      model_config=model_config, is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(\n",
    "      model=detection_model)\n",
    "\n",
    "print(detection_model)\n",
    "print(filenames)\n",
    "ckpt.restore(os.path.join(str(filenames[-1]).replace('.index','')))\n",
    "\n",
    "\n",
    "def get_model_detection_function(model):\n",
    "  \"\"\"Get a tf.function for detection.\"\"\"\n",
    "\n",
    "  @tf.function\n",
    "  def detect_fn(image):\n",
    "    \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "    image, shapes = model.preprocess(image)\n",
    "    prediction_dict = model.predict(image, shapes)\n",
    "    detections = model.postprocess(prediction_dict, shapes)\n",
    "    \n",
    "    \n",
    "    return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
    "\n",
    "  return detect_fn\n",
    "\n",
    "detect_fn = get_model_detection_function(detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Adidas': 1, 'AppleInc': 2, 'MercedesBenz': 3, 'NFL': 4, 'Nike': 5, 'Puma': 6, 'Starbucks': 7, 'TheNorthFace': 8, 'UnderArmour': 9}\n"
     ]
    }
   ],
   "source": [
    "#map labels for inference decoding\n",
    "label_map_path = f\"{current_working_directory}/inference_PROD/{model}/label_map/logos_label_map.pbtxt\"\n",
    "#label_map_path = configs['eval_input_config'].label_map_path\n",
    "label_map = label_map_util.load_labelmap(label_map_path)\n",
    "categories = label_map_util.convert_label_map_to_categories(\n",
    "    label_map,\n",
    "    max_num_classes=label_map_util.get_max_label_map_index(label_map),\n",
    "    use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)\n",
    "\n",
    "print(label_map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#csv is the previous file saved\n",
    "\n",
    "#possibilitÃ : \n",
    "#   1) creazione di folder in base al threshold ex. thrs = 0.5/ 0.8 ...\n",
    "#   2) tutto nella stessa directory\n",
    "# counter pre fixed is needed to understand if we need to create a new or taking and existed one\n",
    "\n",
    "def build_predicion_CSV (threshold , name_image , counter, detections, category):\n",
    "    \n",
    "    sub = re.compile(f\"{current_working_directory}/inference_PROD/INFERENCE_DIR/\")\n",
    "    name_image = re.sub(sub,\"\",name_image)\n",
    "    \n",
    "    info = [\"filename\",\"class\",\"probability\",\"ymin\",\"xmin\",\"ymax\",\"xmax\"]\n",
    "\n",
    "    accuracy = detections['detection_scores'][0].numpy()\n",
    "    boxes = detections['detection_boxes'][0].numpy()\n",
    "    classes = (detections['detection_classes'][0].numpy() + 1).astype(int)\n",
    "    mask_accuracy = accuracy >= threshold\n",
    "\n",
    "    true_accuracy = accuracy[mask_accuracy]\n",
    "    true_boxes = boxes[mask_accuracy, :]\n",
    "    true_classes = classes[mask_accuracy]\n",
    "\n",
    "    if counter == 0:\n",
    "        df_csv = pd.DataFrame(columns=info)\n",
    "        \n",
    "    else:\n",
    "        df_csv = pd.read_csv(f\"{current_working_directory}/inference_PROD/ANNOTATIONS&PREDICTIONS/prediction_bounding_boxes_{int(threshold*100)}.csv\")\n",
    "        df_csv = df_csv\n",
    "    \n",
    "    lenght = len(true_accuracy)\n",
    "    for num in range(lenght):\n",
    "        collection_image={}\n",
    "        collection_image[name_image]={}\n",
    "        collection_image[name_image][num]={}\n",
    "        collection_image[name_image][num][\"label\"] = true_classes[num]  \n",
    "        collection_image[name_image][num][\"accuracy\"] = true_accuracy[num] \n",
    "        collection_image[name_image][num][\"prediction\"] = [true_boxes[num,:][0],true_boxes[num,:][1],true_boxes[num,:][2],true_boxes[num,:][3]]\n",
    "        print(collection_image)\n",
    "        \n",
    "        dict_for_df = [\n",
    "            name_image,\n",
    "            category[collection_image[name_image][num][\"label\"]][\"name\"],\n",
    "            collection_image[name_image][num][\"accuracy\"],\n",
    "            collection_image[name_image][num][\"prediction\"][0],\n",
    "            collection_image[name_image][num][\"prediction\"][1],\n",
    "            collection_image[name_image][num][\"prediction\"][2],\n",
    "            collection_image[name_image][num][\"prediction\"][3]\n",
    "        ]\n",
    "        \n",
    "        element = pd.DataFrame([dict_for_df], columns=info)\n",
    "       \n",
    "        df_csv = pd.concat([df_csv,element],ignore_index=True)\n",
    "                \n",
    "                \n",
    "    df_csv.to_csv(f\"{current_working_directory}/inference_PROD/ANNOTATIONS&PREDICTIONS/prediction_bounding_boxes_{int(threshold*100)}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import time\n",
    "\n",
    "def print_true_predicted(image):\n",
    "\n",
    "    if \"COMPARE\" not in os.listdir(os.getcwd()):\n",
    "        os.mkdir(os.path.join(os.getcwd(), \"COMPARE\"))\n",
    "\n",
    "    image_path = image\n",
    "    image_np = load_image_into_numpy_array(image_path)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(\n",
    "        np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "    threshold = 0.2\n",
    "    \n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                  image_np_with_detections,\n",
    "                  detections['detection_boxes'][0].numpy(),\n",
    "                  (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
    "                  detections['detection_scores'][0].numpy(),\n",
    "                  category_index,\n",
    "                  use_normalized_coordinates=True, # Needs to be true cause otherwise boxes go out of range\n",
    "                  max_boxes_to_draw=3, # Minimized to 10 because otherwise we might have troubles \n",
    "                  min_score_thresh=threshold, # Minimized the score, as for now it seems to be too confident over Adidas\n",
    "                  agnostic_mode=False)\n",
    "    \n",
    "    df_general_annot = pd.read_csv(f\"{current_working_directory}/inference_PROD/ANNOTATIONS&PREDICTIONS/annotations.csv\")\n",
    "    simple_filename = image.split(\"\\\\\")[-1]\n",
    "    df_true_annot = df_general_annot[df_general_annot[\"filename\"]==simple_filename]\n",
    "\n",
    "    plt.figure(figsize=(12,16))\n",
    "\n",
    "    for index, row in df_true_annot.iterrows():\n",
    "        # Create a Rectangle patch\n",
    "        plt.scatter(row[\"xmin\"], row[\"ymin\"], color=\"red\")\n",
    "        plt.scatter(row[\"xmin\"], row[\"ymax\"], color=\"red\")\n",
    "        plt.scatter(row[\"xmax\"], row[\"ymin\"], color=\"red\")\n",
    "        plt.scatter(row[\"xmax\"], row[\"ymax\"], color=\"red\")\n",
    "        plt.text((row[\"xmax\"]+row[\"xmin\"])//2, (row[\"ymax\"]+row[\"ymin\"])//2, row[\"class\"], fontsize=18, color=\"white\", fontweight=\"bold\")\n",
    "\n",
    "    plt.imshow(image_np_with_detections)\n",
    "    plt.savefig(f\"{current_working_directory}/inference_PROD/COMPARE/{simple_filename}\")\n",
    "    plt.close()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the plotting of the detections for each image\n",
    "\n",
    "counter = 0\n",
    "\n",
    "TEST_IMAGE_PATHS = glob.glob(f'{current_working_directory}/inference_PROD/INFERENCE_DIR/*.jpg')\n",
    "images_list = list(TEST_IMAGE_PATHS)\n",
    "for img in images_list:\n",
    "    print(img)\n",
    "    print_true_predicted(img, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40a8aae49d8ead891f177a9aa92f4d86d5628cb1b4814a86ede39a8e1ef554b8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
